{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f3e6af",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "\n",
    "1. Change the datapath and load the data (For the scatter plot we are working with the data with 48 brain regions and a threshold of 0.5, the full set of data is available in the Drive in the Folder Split_48_Neutre, be careful to use these data, there was a mistake in previous ones)\n",
    "\n",
    "\n",
    "2. Run the preprocessing (Both steps)\n",
    "\n",
    "\n",
    "3. Train one of the models, skip the others\n",
    "\n",
    "\n",
    "4. Run the two cells of the section Analysis of Prediction, they will return a table with all accuracies per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa634043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74824413",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed59375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------data paths------------------------------------------------------------------------------------------\n",
    "data_train_path = 'C:/Users/manon/Desktop/Projet_2 _ML/code/datas/Split_48_neutre/Combined_All_Train_80.csv'\n",
    "data_test_path = 'C:/Users/manon/Desktop/Projet_2 _ML/code/datas/Split_48_neutre/Combined_All_Test_80.csv'\n",
    "\n",
    "#--------loading the data and spliting between features and predictions--------------------------------------\n",
    "tx_train = pd.read_csv(data_train_path,sep=\",\",squeeze=True)\n",
    "X_train=tx_train.iloc[:, :48]\n",
    "ytr = pd.read_csv(data_train_path,sep=\",\",usecols=[48],squeeze=True)\n",
    "\n",
    "tx_test = pd.read_csv(data_test_path,sep=\",\",squeeze=True)\n",
    "X_test=tx_test.iloc[:, :48]\n",
    "yte = pd.read_csv(data_test_path,sep=\",\",usecols=[48],squeeze=True)\n",
    "\n",
    "#---------Adding features name (i.e. number of the brain region represented by the feature)------------------\n",
    "def add_column_names(data):\n",
    "    \n",
    "    liste=[]\n",
    "    for i in range(48):\n",
    "        liste.append(str(i+1))\n",
    "    data.columns=liste\n",
    "    return data,liste\n",
    "\n",
    "X_train,liste = add_column_names(X_train)\n",
    "X_test,liste = add_column_names(X_test)\n",
    "\n",
    "#---------Convert y to int to be compatible with future prediction--------------------------------------------\n",
    "ytr = ytr.astype(np.int64)\n",
    "yte = yte.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4fe63",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "###  1. Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e8ab8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf3e9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=standardize(X_train)\n",
    "X_test=standardize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f68973",
   "metadata": {},
   "source": [
    "### 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30038118",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e4584",
   "metadata": {},
   "source": [
    "# Training\n",
    "### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b6a8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.330603889457523\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 500, criterion = 'gini', max_depth=9,random_state = 42)\n",
    "classifier.fit(X_train, ytr)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(yte, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e8cfb",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0bac5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.283179802115319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 4, criterion='gini').fit(X_train, ytr)\n",
    "dtree_predictions = dtree_model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(yte, dtree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413e710",
   "metadata": {},
   "source": [
    "### 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9be8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2811327192084613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 0.025).fit(X_train, ytr) #c=0.02 for linear,0.33 for poly, 0.01 for sigmoid\n",
    "svm_predictions = svm_model_linear.predict(X_test)\n",
    "accuracy = svm_model_linear.score(X_test, yte)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e659a0",
   "metadata": {},
   "source": [
    "### 4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b8c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2739679290344592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 873, weights='distance').fit(X_train, ytr)\n",
    "accuracy = knn.score(X_test, yte)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccd7b6",
   "metadata": {},
   "source": [
    "### 5. Logistic Regression (if we decide to keep it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7406fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2517911975435005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver = 'saga',multi_class='auto')\n",
    "logreg.fit(X_train,ytr)\n",
    "accuracy = logreg.score(X_test, yte)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cae675",
   "metadata": {},
   "source": [
    "### 6. Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcec2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=tf.expand_dims(X_train, axis=-1)\n",
    "X_test=tf.expand_dims(X_test, axis=-1)\n",
    "y_train = tf.keras.utils.to_categorical(ytr,num_classes=14)\n",
    "y_test = tf.keras.utils.to_categorical(yte,num_classes=14)\n",
    "\n",
    "n_samples, n_features = X_train.shape[0], X_train.shape[1]\n",
    "n_outputs=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a64b72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Conv1D(filters=12, kernel_size=5, activation='tanh',kernel_initializer='he_uniform',input_shape=(n_features, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='tanh',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(60, input_dim=199, activation='tanh'))\n",
    "model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "478ca7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    epochs, batch_size = 10, 1\n",
    "    n_samples, n_features = X_train.shape[0], X_train.shape[1]\n",
    "    n_outputs=14\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2afe4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8910/8910 [==============================] - 30s 3ms/step - loss: 6.4384 - accuracy: 0.2369\n",
      "Epoch 2/10\n",
      "8910/8910 [==============================] - 30s 3ms/step - loss: 6.3695 - accuracy: 0.2387\n",
      "Epoch 3/10\n",
      "8910/8910 [==============================] - 30s 3ms/step - loss: 6.2926 - accuracy: 0.2439\n",
      "Epoch 4/10\n",
      "8910/8910 [==============================] - 31s 3ms/step - loss: 6.3142 - accuracy: 0.2566 0s - loss: 6.3129 - accuracy: 0.25\n",
      "Epoch 5/10\n",
      "8910/8910 [==============================] - 31s 4ms/step - loss: 5.9908 - accuracy: 0.2678\n",
      "Epoch 6/10\n",
      "8910/8910 [==============================] - 31s 4ms/step - loss: 6.0022 - accuracy: 0.2827\n",
      "Epoch 7/10\n",
      "8910/8910 [==============================] - 31s 3ms/step - loss: 5.9649 - accuracy: 0.2842\n",
      "Epoch 8/10\n",
      "8910/8910 [==============================] - 32s 4ms/step - loss: 5.9424 - accuracy: 0.2896\n",
      "Epoch 9/10\n",
      "8910/8910 [==============================] - 31s 4ms/step - loss: 5.9384 - accuracy: 0.2890\n",
      "Epoch 10/10\n",
      "8910/8910 [==============================] - 30s 3ms/step - loss: 5.9497 - accuracy: 0.2927\n",
      "2931/2931 [==============================] - 6s 2ms/step - loss: 6.2469 - accuracy: 0.2668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26680314540863037"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be7da3",
   "metadata": {},
   "source": [
    "### 7. Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69dc4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=tf.expand_dims(X_train, axis=-1)\n",
    "X_test=tf.expand_dims(X_test, axis=-1)\n",
    "y_train = tf.keras.utils.to_categorical(ytr,num_classes=14)\n",
    "y_test = tf.keras.utils.to_categorical(yte,num_classes=14)\n",
    "\n",
    "n_samples, n_features = X_train.shape[0], X_train.shape[1]\n",
    "n_outputs=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16701cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(6, input_dim=48, activation='relu',kernel_initializer='zero'))\n",
    "model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adagrad(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1426371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    epochs, batch_size = 10, 100\n",
    "    n_samples, n_features = X_train.shape[0], X_train.shape[1]\n",
    "    n_outputs=14\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "124d1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8910/8910 [==============================] - 51s 5ms/step - loss: 6.4135 - accuracy: 0.2242\n",
      "Epoch 2/10\n",
      "8910/8910 [==============================] - 52s 6ms/step - loss: 5.4890 - accuracy: 0.2371 0s - loss: 5.4910 - accuracy: 0.\n",
      "Epoch 3/10\n",
      "8910/8910 [==============================] - 48s 5ms/step - loss: 5.4873 - accuracy: 0.2371\n",
      "Epoch 4/10\n",
      "8910/8910 [==============================] - 50s 6ms/step - loss: 5.6316 - accuracy: 0.2371\n",
      "Epoch 5/10\n",
      "8910/8910 [==============================] - 49s 5ms/step - loss: 5.7393 - accuracy: 0.2371 0s - loss: 5.7462 - \n",
      "Epoch 6/10\n",
      "8910/8910 [==============================] - 50s 6ms/step - loss: 5.7385 - accuracy: 0.2371\n",
      "Epoch 7/10\n",
      "8910/8910 [==============================] - 49s 5ms/step - loss: 5.7388 - accuracy: 0.2371\n",
      "Epoch 8/10\n",
      "8910/8910 [==============================] - 53s 6ms/step - loss: 5.7382 - accuracy: 0.2371\n",
      "Epoch 9/10\n",
      "8910/8910 [==============================] - 55s 6ms/step - loss: 5.7388 - accuracy: 0.2371\n",
      "Epoch 10/10\n",
      "8910/8910 [==============================] - 55s 6ms/step - loss: 5.7387 - accuracy: 0.2371\n",
      "2931/2931 [==============================] - 10s 3ms/step - loss: 5.6146 - accuracy: 0.2648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26475605368614197"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796ffee",
   "metadata": {},
   "source": [
    "# Analysis of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0441541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yte = np.expand_dims(yte, axis=1)\n",
    "#Y_pred = np.expand_dims(Y_pred, axis=1)\n",
    "Yte=pd.DataFrame(Yte,columns = ['Emotions'])\n",
    "Y_pred=pd.DataFrame(Y_pred,columns = ['Emotions'])\n",
    "\n",
    "dict = pd.DataFrame({0:'Anger',1:'Sad',2:'Guilt',3:'Shame',4:'Disgust',5:'Anxiety',6:'Fear',7:'Surprise',8:'Contempt',9:'Satisfaction',\n",
    "            10:'WarmHeart.',11:'Happiness',12:'Love',13:'Neutral'}, index=[0])\n",
    "\n",
    "Yte=Yte.replace({\"Emotions\": dict})\n",
    "Y_pred=Y_pred.replace({\"Emotions\": dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82c6cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.41      0.25      0.31        60\n",
      "     Anxiety       0.32      0.05      0.09       148\n",
      "    Contempt       0.66      0.09      0.15       240\n",
      "     Disgust       0.56      0.20      0.29       192\n",
      "        Fear       0.23      0.04      0.08       156\n",
      "   Happiness       0.32      0.79      0.46       776\n",
      "        Love       0.21      0.05      0.08       104\n",
      "     Neutral       0.44      0.37      0.40       208\n",
      "         Sad       0.38      0.24      0.29       168\n",
      "Satisfaction       0.21      0.15      0.17       315\n",
      "       Shame       0.32      0.27      0.30       304\n",
      "    Surprise       0.41      0.19      0.26        72\n",
      "  WarmHeart.       0.12      0.01      0.01       188\n",
      "\n",
      "    accuracy                           0.33      2931\n",
      "   macro avg       0.35      0.21      0.22      2931\n",
      "weighted avg       0.35      0.33      0.27      2931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Yte, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
